---
---

@article{ibrahimzada2024program,
	title = {Program Decomposition and Translation with Static Analysis},
	abstract = {The rising popularity of Large Language Models (LLMs) has motivated exploring their use in code-related tasks. Code LLMs with more than millions of parameters are trained on a massive amount of code in different Programming Languages (PLs). Such models are used for automating various Software Engineering (SE) tasks using prompt engineering. However, given the very large size of industry-scale project files, a major issue of these LLMs is their limited context window size, motivating the question of "Can these LLMs process very large files and can we effectively perform prompt engineering?". Code translation aims to convert source code from one PL to another. In this work, we assess the effect of method-level program decomposition on context window of LLMs and investigate how this approach can enable translation of very large files which originally could not be done due to out-of-context issue. Our observations from 20 well-known java projects and approximately 60K methods suggest that method-level program decomposition significantly improves the limited context window problem of LLMs by 99.5%. Furthermore, our empirical analysis indicate that with method-level decomposition, each input fragment on average only consumes 5% of the context window, leaving more context space for prompt engineering and the output. Finally, we investigate the effectiveness of a Call Graph (CG) approach for translating very large files when doing method-level program decomposition.},
	journal = {In Proceedings of 46th IEEE/ACM International Conference on Software Engineering Companion (ICSE ‚Äô24 Companion), April 14-20, 2024, Lisbon, Portugal},
	author = {Ibrahimzada, Ali Reza},
	month = april,
	year = {2024},
	selected = {true},
	abbr = {ICSE'24 SRC},
	arxiv = {2401.12412},
	html = {https://doi.org/10.1145/3639478.3641226},
	pdf = {decomposition.pdf},
}

@article{ibrahimzada2023automated,
      title={Automated Bug Generation in the era of Large Language Models},
	  abstract = {Bugs are essential in software engineering; many research studies in the past decades have been proposed to detect, localize, and repair bugs in software systems. Effectiveness evaluation of such techniques requires complex bugs, i.e., those that are hard to detect through testing and hard to repair through debugging. From the classic software engineering point of view, a hard-to-repair bug differs from the correct code in multiple locations, making it hard to localize and repair. Hard-to-detect bugs, on the other hand, manifest themselves under specific test inputs and reachability conditions. These two objectives, i.e., generating hard-to-detect and hard-to-repair bugs, are mostly aligned; a bug generation technique can change multiple statements to be covered only under a specific set of inputs. However, these two objectives are conflicting for learning-based techniques: A bug should have a similar code representation to the correct code in the training data to challenge a bug prediction model to distinguish them. The hard-to-repair bug definition remains the same but with a caveat: the more a bug differs from the original code (at multiple locations), the more distant their representations are and easier to be detected. We propose BugFarm, to transform arbitrary code into multiple complex bugs. BugFarm leverages Large Language Models (LLMs) to mutate code in multiple locations (hard-to-repair). To ensure that multiple modifications do not notably change the code representation, BugFarm analyzes the attention of the underlying model and instructs LLMs to only change the least attended locations (hard-to-detect). Our comprehensive evaluation of 320K+ bugs from over 2.5M mutants generated by BugFarm and two alternative approaches demonstrates our superiority in generating bugs that are hard to detect by learning-based bug prediction approaches (up to 41% higher False Negative Rate and 11%, 6%, 29%, and 21% lower Accuracy, Precision, Recall, and F1 score) and hard to repair by state-of-the-art learning-based program repair technique (22% repair success rate compared to 34% and 49% of LEAM and ùúáBERT bugs). BugFarm is efficient, i.e., it takes nine seconds for it to mutate a code without any prior training time.}, 
      author={Ali Reza Ibrahimzada and Yang Chen and Ryan Rong and Reyhaneh Jabbarvand},
      year={2023},
	  month= oct,
	  selected = {true},
	  abbr = {pre-print},
	  pdf = {bugfarm.pdf},
	  arxiv = {2310.02407},
 	  journal = {arxiv preprint arXiv:2310.02407},
}

@article{pan2024lost,
	title = {Lost in Translation: A Study of Bugs Introduced by Large Language Models while Translating Code},
	abstract = {Code translation aims to convert source code from one programming language (PL) to another. Given the promising abilities of large language models (LLMs) in code synthesis, researchers are exploring their potential to automate code translation. The prerequisite for advancing the state of LLM-based code translation is to understand their promises and limitations over existing techniques. To that end, we present a large-scale empirical study to investigate the ability of general LLMs and code LLMs for code translation across pairs of different languages, including C, C++, Go, Java, and Python. Our study, which involves the translation of 1,700 code samples from three benchmarks and two real-world projects, reveals that LLMs are yet to be reliably used to automate code translation---with correct translations ranging from 2.1% to 47.3% for the studied LLMs. Further manual investigation of unsuccessful translations identifies 15 categories of translation bugs. We also compare LLM-based code translation with traditional non-LLM-based approaches. Our analysis shows that these two classes of techniques have their own strengths and weaknesses. Finally, insights from our study suggest that providing more context to LLMs during translation can help them produce better results. To that end, we propose a prompt-crafting approach based on the symptoms of erroneous translations; this improves the performance of LLM-based code translation by 5.5% on average. Our study is the first of its kind, in terms of scale and breadth, that provides insights into the current limitations of LLMs in code translation and opportunities for improving them. Our dataset---consisting of 1,700 code samples in five PLs with 10K+ tests, 43K+ translated code, 1,725 manually labeled bugs, and 1,365 bug-fix pairs---can help drive research in this area.},
	journal = {In Proceedings of 46th IEEE/ACM International Conference on Software Engineering (ICSE ‚Äô24), April 14-20, 2024, Lisbon, Portugal},
	author = {Pan, Rangeet and Ibrahimzada, Ali Reza and Krishna, Rahul and Sankar, Divya and Wassi, Lambert Pouguem and Merler, Michele and Sobolev, Boris and Pavuluri, Raju and Sinha, Saurabh and Jabbarvand, Reyhaneh},
	month = april,
	year = {2024},
	selected = {true},
	badge_avail = {true},
	badge_reus = {true},
	code = {https://github.com/Intelligent-CAT-Lab/PLTranslationEmpirical},
	html = {https://doi.org/10.1145/3597503.3639226},
	abbr = {ICSE'24},
	pdf = {plempirical.pdf},
	slides = {https://uofi.box.com/s/x35hq921p5ct41fcvsr3jgq31osqzkrk},
	arxiv = {2308.03109}
}

@article{ibrahimzada2022perfect,
	title = {Perfect Is the Enemy of Test Oracle},
	abstract = {Automation of test oracles is one of the most challenging facets of software testing, but remains comparatively less addressed compared to automated test input generation. Test oracles rely on a ground-truth that can distinguish between the correct and buggy behavior to determine whether a test fails (detects a bug) or passes. What makes the oracle problem challenging and undecidable is the assumption that the ground-truth should know the exact expected, correct or buggy behavior. However, we argue that one can still build an accurate oracle without knowing the exact correct or buggy behavior, but how these two might differ. This paper presents SEER, a Deep Learning-based approach that in the absence of test assertions or other types of oracle, can automatically determine whether a unit test passes or fails on a given method under test (MUT). To build the ground-truth, SEER jointly embeds unit tests and the implementation of MUTs into a unified vector space, in such a way that the neural representation of tests are similar to that of MUTs they pass on them, but dissimilar to MUTs they fail on them. The classifier built on top of this vector representation serves as the oracle to generate ‚Äúfail‚Äù labels, when test inputs detect a bug in MUT or ‚Äúpass‚Äù labels, otherwise. Our extensive experiments on applying SEER to more than 5K unit tests from a diverse set of opensource Java projects show that the produced oracle is (1) effective in predicting the fail or pass labels, achieving an overall accuracy, precision, recall, and F1 measure of 93%, 86%, 94%, and 90%, (2) generalizable, predicting the labels for the unit test of projects that were not in training or validation set with negligible performance drop, and (3) efficient, detecting the existence of bugs in only 6.5 milliseconds on average. Moreover, by interpreting the neural model and looking at it beyond a closed-box solution, we confirm that the oracle is valid, i.e., it predicts the labels through learning relevant features.},
	journal = {In Proceedings of the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering (ESEC/FSE ‚Äô22), November 14‚Äì18, 2022, Singapore, Singapore},
	author = {Ibrahimzada, Ali Reza and Varli, Yigit and Tekinoglu, Dilara and Jabbarvand, Reyhaneh},
	month = june,
	year = {2022},
	selected = {true},
	abbr = {ESEC/FSE'22},
	badge_avail = {true},
	badge_func = {true},
	code = {https://github.com/Intelligent-CAT-Lab/SEER},
	html = {https://doi.org/10.1145/3540250.3549086},
	video = {https://www.youtube.com/watch?v=k9060RSH0hg},
	pdf = {seer.pdf},
	slides = {https://uofi.box.com/s/qxsskoer8iil0c47aptxg478h0s0jhz5},
	arxiv = {2302.01488}
}

@Article{Cakmak2023,
	author={Cakmak, Ali
	and Ayaz, Huzeyfe
	and Ar{\i}kan, Soykan
	and Ibrahimzada, Ali Reza
	and Demirkol, {\c{S}}eyda
	and S{\"o}nmez, Dilara
	and Hakan, Mehmet T.
	and S{\"u}rmen, Saime T.
	and Horozo{\u{g}}lu, Cem
	and Do{\u{g}}an, Mehmet B.
	and K{\"u}{\c{c}}{\"u}kh{\"u}seyin, {\"O}zlem
	and Cac{\i}na, Canan
	and K{\i}ran, Bayram
	and Zeybek, {\"U}mit
	and Baysan, Mehmet
	and Yayl{\i}m, {\.{I}}lhan},
	title={Predicting the predisposition to colorectal cancer based on SNP profiles of immune phenotypes using supervised learning models},
	journal={Medical {\&} Biological Engineering {\&} Computing, Springer Berlin Heidelberg, Vol. 61, 243‚Äì258, 2023},
	year={2023},
	month=jan,
	day={01},
	volume={61},
	number={1},
	pages={243-258},
	abstract={This study explores the machine learning-based assessment of predisposition to colorectal cancer based on single nucleotide polymorphisms (SNP). Such a computational approach may be used as a risk indicator and an auxiliary diagnosis method that complements the traditional methods such as biopsy and CT scan. Moreover, it may be used to develop a low-cost screening test for the early detection of colorectal cancers to improve public health. We employ several supervised classification algorithms. Besides, we apply data imputation to fill in the missing genotype values. The employed dataset includes SNPs observed in particular colorectal cancer-associated genomic loci that are located within DNA regions of 11 selected genes obtained from 115 individuals. We make the following observations: (i) random forest-based classifier using one-hot encoding and K-nearest neighbor (KNN)-based imputation performs the best among the studied classifiers with an F1 score of 89{\%} and area under the curve (AUC) score of 0.96. (ii) One-hot encoding together with K-nearest neighbor-based data imputation increases the F1 scores by around 26{\%} in comparison to the baseline approach which does not employ them. (iii) The proposed model outperforms a commonly employed state-of-the-art approach, ColonFlag, under all evaluated settings by up to 24{\%} in terms of the AUC score. Based on the high accuracy of the constructed predictive models, the studied 11 genes may be considered a gene panel candidate for colon cancer risk screening.},
	issn={1741-0444},
	doi={10.1007/s11517-022-02707-9},
	url={https://doi.org/10.1007/s11517-022-02707-9},
	selected = {false},
	code = {https://github.com/itu-bioinformatics-database-lab/CRC_Prediction_with_Immune_SNP_Profiles},
	html = {https://doi.org/10.1007/s11517-022-02707-9},
	abbr = {MBEC},
	pdf = {colorectal.pdf},
}


@article{doi:10.1142/S0219720020500262,
	title = {Scalable classification of organisms into a taxonomy using hierarchical supervised learners},
	volume = {18},
	issn = {0219-7200},
	url = {https://www.worldscientific.com/doi/abs/10.1142/S0219720020500262},
	html = {https://www.worldscientific.com/doi/abs/10.1142/S0219720020500262},
	doi = {10.1142/S0219720020500262},
	abstract = {Accurately identifying organisms based on their partially available genetic material is an important task to explore the phylogenetic diversity in an environment. Specific fragments in the DNA sequence of a living organism have been defined as DNA barcodes and can be used as markers to identify species efficiently and effectively. The existing DNA barcode-based classification approaches suffer from three major issues: (i) most of them assume that the classification is done within a given taxonomic class and/or input sequences are pre-aligned, (ii) highly performing classifiers, such as SVM, cannot scale to large taxonomies due to high memory requirements, (iii) mutations and noise in input DNA sequences greatly reduce the taxonomic classification score. In order to address these issues, we propose a multi-level hierarchical classifier framework to automatically assign taxonomy labels to DNA sequences. We utilize an alignment-free approach called spectrum kernel method for feature extraction. We build a proof-of-concept hierarchical classifier with two levels, and evaluated it on real DNA sequence data from barcode of life data systems. We demonstrate that the proposed framework provides higher f1-score than regular classifiers. Besides, hierarchical framework scales better to large datasets enabling researchers to employ classifiers with high classification performance and high memory requirement on large datasets. Furthermore, we show that the proposed framework is more robust to mutations and noise in sequence data than the non-hierarchical classifiers.},
	number = {05},
	urldate = {2021-07-08},
	journal = {Journal of Bioinformatics and Computational Biology, World Scientific Publishing Co., Vol. 18, No. 05, 2020},
	author = {Sohsah, Gihad N. and Ibrahimzada, Ali Reza and Ayaz, Huzeyfe and Cakmak, Ali},
	month = oct,
	year = {2020},
	pages = {2050026},
	selected = {false},
	note ={PMID: 33125294},
	URL = {https://doi.org/10.1142/S0219720020500262},
	eprint = {https://doi.org/10.1142/S0219720020500262},
	abbr = {JBCB},
	pdf = {Scalable Classification of Organisms into a Taxonomy Using Hierarchical Supervised Learners.pdf},
	code = {https://github.com/itu-bioinformatics-database-lab/Hierarchical-Supervised-Learners}
}

@article{cakmak_2022,
	title = {Predicting the Predisposition to Colorectal Cancer based on SNP Profiles of Immune Checkpoints Using Supervised Learning Models},
	journal = {VII. International Molecular Medicine Congress, September 2019, Istanbul, Turkey},
	author = {Cakmak, Ali and Ibrahimzada, Ali Reza and Arikan, Soykan and Ayaz, Huzeyfe and Demirkol, Seyda and Sonmez, Dilara and Hakan, Mehmet Tolgahan and Surmen, Saime Turan and Horozoglu, Cem and Kucukhuseyin, Ozlem and Cacina, Canan and Kiran, Bayram and Zeybek, Umit and Baysan, Mehmet and Yaylim, Ilhan},
	month = sep,
	year = {2019},
	selected = {false},
	abbr = {IMMC}
}
